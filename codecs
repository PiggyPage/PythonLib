[Python标准库]codecs——字符串编码和解码
    作用：编码器和解码器，用于转换文本的不同表示。
    Python 版本：2.1 及以后版本
    codecs 模块提供了流接口和文件接口来转换数据。通常用于处理 Unicode 文本，不过也提供了其他编码来满足其他用途。
Unicode 入门
    CPython 2.x 支持两种类型的字符串来处理文本数据。老式的 str 实例使用单个 8 位字节表示字符串字符（使用其 ASCII 码）。与之不同，unicode 串在内部作为一个 Unicode 码点（code point）序列来管理。码点值分别保存为两个或四个字节的序列，这取决于编译 Python 时指定的选项。unicode 和 str 都派生自一个公共基类，并支持类似的 API。
    输出 unicode 串时，会使用某种标准机制编码，使得以后可以将这个字节序列重构为同样的文本串。编码值的字节不一定与码点值完全相同，编码只是定义了在两个值集之间转换的一种方式。读取 Unicode 数据时还需要指定编码，这样才能把接收到的字节转换为 unicode 类使用的内部表示。
    西方语言最常用的编码是 UTF-8 和 UTF-16，这两种编码分别使用单字节和两字节值序列表示各个码点。对于其他语言，由于大多数字符都由超过两字节的码点表示，所以使用其他编码来存储可能更为高效。
    编码
    要了解编码，最好的方法就是采用不同方式对相同的串进行编码，并查看所生成的字节序列的区别。下面的例子使用以下函数格式化字符串，使之更易读。

import binascii

def to_hex(t, nbytes):
    """Format text t as a sequence of nbyte long values
    separated by spaces.
    """
    chars_per_item = nbytes * 2
    hex_version = binascii.hexlify(t)
    return ' '.join(
        hex_version[start:start + chars_per_item]
        for start in xrange(0, len(hex_version), chars_per_item)
        )

if __name__ == '__main__':
    print to_hex('abcdef', 1)
    print to_hex('abcdef', 2)

    这个函数使用 binascii 得到输入字节串的十六进制表示，在返回这个值之前每隔 nbytes 字节就插入一个空格。
    第一个编码示例首先使用 unicode 类的原始表示打印文本 pi：π，π 字符替换为其 Unicode 码点的表达式 \u03c0。接下来的两行将这个字符串分别编码为 UTF-8 和 UTF-16，并显示编码得到的十六进制值。

from codecs_to_hex import to_hex

text = u'pi: π'

print 'Raw   :', repr(text)
print 'UTF-8 :', to_hex(text.encode('utf-8'), 1)
print 'UTF-16:', to_hex(text.encode('utf-16'), 2)

    对一个 unicode 串编码的结果是一个 str 对象。
    给定一个编码字节序列（作为一个 str 实例），decode() 方法会将其转换为码点，并作为一个 unicode 实例返回转换后的序列。

#coding=utf-8
from codecs_to_hex import to_hex

text = u'pi: π'
encoded = text.encode('utf-8')
decoded = encoded.decode('utf-8')

print 'Original :', repr(text)
print 'Encoded  :', to_hex(encoded, 1), type(encoded)
print 'Decoded  :', repr(decoded), type(decoded)

    选择使用哪一种编码不会改变输出类型。
处理文件
    处理 I/O 操作时，编码和解码字符串尤其重要。不论是写至一个文件、一个套接字还是另一个流，数据都必须使用适当的编码。一般来讲，所有文本数据在读取时都需要从其字节表示解码，写时则需要从内部值编码为一种特定的表示。一个程序可以显式地编码和解码数据，但是取决于所用的编码，要确定是否已读取足够的字节以便充分解码数据，这一点可能并不容易。codecs 提供了一些类来管理数据的编码和解码，所以应用不需要做这个工作。
    codecs 提供的最简单的接口可以用来替代内置 open() 函数。这个新版本的函数与内置函数的做法很相似，不过增加了两个参数来指定编码和所需的错误处理技术。

from codecs_to_hex import to_hex

import codecs
import sys

encoding = sys.argv[1]
filename = encoding + '.txt'

print 'Writing to', filename
with codecs.open(filename, mode='wt', encoding=encoding) as f:
    f.write(u'pi: \u03c0')

# Determine the byte grouping to use for to_hex()
nbytes = { 'utf-8':1,
           'utf-16':2,
           'utf-32':4,
           }.get(encoding, 1)
# Show the raw bytes in the file
print 'File contents:'
with open(filename, mode='rt') as f:
    print to_hex(f.read(), nbytes)

    这个例子首先从一个 unicode 串开始（包含 π 的码点），并使用命令行上指定的编码将文件保存到一个文件。
    用 open() 读数据很简单，但有一点要注意：必须提前知道编码，才能正确地建立解码器。尽管有些数据格式（如 XML）会指定编码作为文件的一部分，但是通常都要由应用来管理。codecs 只是取一个编码参数，并假设这个编码是正确的。

import codecs
import sys

encoding = sys.argv[1]
filename = encoding + '.txt'

print 'Reading from', filename
with codecs.open(filename, mode='rt', encodind=encoding) as f:
    print repr(f.read())
    
    这个例子读取上一个程序创建的文件，并把得到的 unicode 对象的表示打印到控制台。
字节序
    在不同计算机系统之间传输数据时（可能直接复制一个文件，或者使用网络通信来完成传输），多字节编码（如 UTF-16 和 UTF-32）会引发一个问题。不同系统中使用的高字节和低字节的顺序不同。数据的这个特性，称为字节序（endianness），它依赖于硬件体系结构等因素，还取决于操作系统和应用开发人员做出的选择。通常没有办法提前知道给定的一组数据要使用哪一种字节序，所以多字节编码还包括一个字节序标志（byte-order，BOM），这个标志出现在编码输出的前几个字节。例如，UTF-16 定义 0xFFFE 和 0xFEFF 不是合法字符，而是用于显示字节序。codecs 定义了 UTF-16 和 UTF-32 所用字节序标志的相应常量。

import codecs
from codecs_to_hex import to_hex

for name in [ 'BOM', 'BOM_BE', 'BOM_LE',
              'BOM_UTF8',
              'BOM_UTF16', 'BOM_UTF16_BE', 'BOM_UTF16_LE',
              'BOM_UTF32', 'BOM_UTF32_BE', 'BOM_UTF32_LE',
              ]:
    print '{:12} : {}'.format(name, to_hex(getattr(codecs, name), 2))

    取决于当前系统的固有字节序，BOM、BOM_UTF16 和 BOM_UTF32 会自动设置为适当的大端（big-endian）或小端（little-endian）值。
    可以由 codecs 中的解码器自动检测和处理字节序，不过编码时也可以显式地指定字节序。

import codecs
from codecs_to_hex import to_hex

# Pick the nonnative version of UTF-16 encoding
if codecs.BOM_UTF16 == codecs.BOM_UTF16_BE:
    bom = codecs.BOM_UTF16_LE
    encoding = 'utf_16_le'
else:
    bom = codecs.BOM_UTF16_BE
    encoding = 'utf_16_be'

print 'Native order  :', to_hex(codecs.BOM_UTF16, 2)
print 'Selected order:', to_hex(bom, 2)

# Encode the text.
encoded_text = u'pi: \u03c0'.encode(encoding)
print '{:14}: {}'.format(encoding, to_hex(encoded_text, 2))

with open('nonnative-encoded.txt', mode='wb') as f:
    # Write the selected byte-order marker. It id not included
    # in the encoded text because the byte order was given
    # explicitly when selecting the encoding.
    f.write(bom)
    # Write the byte string for the encoded text.
    f.write(encoded_text)

    codecs_bom_create_file.py 首先得出内置的字节序，然后显式地使用候选形式，以便下一个例子可以展示读取时自动检测字节序。
    codecs_bom_detection.py 打开文件时没有指定字节序，所以解码器会使用文件前两个字节中的 BOM 值来确定字节序。

import codecs
from codecs_to_hex import to_hex

# Look at the raw data
with open('nonnative-encoded.txt', mode='rb') as f:
    raw_bytes = f.read()

print 'Raw    :', to_hex(raw_bytes, 2)

# Reopen the file and let codecs detect the BOM
with codecs.open('nonnative-encoded.txt',
                 mode='r',
                 encoding='utf-16',
                 ) as f:
    decoded_text = f.read()

print 'Decoded:', repr(decoded_text)

    由于文件的前两个字节用于字节序检测，所以它们并不包含在 read() 返回的数据中。
错误处理
    前面几节指出，读写 Unicode 文件时需要知道所使用的编码。正确地设置编码很重要，这有两个原因。如果读文件时未能正确地配置编码，就无法正确地解释数据，数据则有可能破坏或者无法解码。并不是所有 Unicode 字符都可以采用所有编码表示，所以如果写文件时使用了错误的编码，就会产生一个错误，数据也可能丢失。
    类似于 unicode 的 encode() 方法和 str 的 decode() 方法，codecs 也使用了同样的 5 个错误处理选项，如下所示：
    |---------------------|-----------------------------------------------|
    |      错误模式       |                   描述                        |
    |---------------------|-----------------------------------------------|
    |  strict             |  如果数据无法转换，产生一个异常               |
    |---------------------|-----------------------------------------------|
    |  replace            |  将无法编码的数据替换为一个特殊的标志字符     |
    |---------------------|-----------------------------------------------|
    |  ignore             |  跳过数据                                     |
    |---------------------|-----------------------------------------------|
    |  xmlcharrefreplace  |  XML 字符（仅适用编码）                       |
    |---------------------|-----------------------------------------------|
    |  backslashreplace   |  转义序列（仅适用编码）                       |
    |---------------------------------------------------------------------|
    1. 编码错误
    最常见的错误条件是向一个 ASCII 输出流（如一个常规文件或 sys.stdout）写 Unicode 数据时接收到一个 UnicodeEncodeError。以下示例程序可以用来试验不同的错误处理模式。

import codecs
import sys

error_handling = sys.argv[1]

text = u'pi: \u03c0'

try:
    # Save the data, encoded as ASCII, using the error
    # handling mode specified on the command line.
    with codecs.open('encode_error.txt', 'w',
                     encoding='ascii',
                     errors=error_handling) as f:
        f.write(text)

except UnicodeEncodeError, err:
    print 'ERROR:', err
else:
    # If there was no error writing to the file,
    # show what it contains.
    with open('encode_error.txt', 'r') as f:
        print 'File contents:', repr(f.read())

    要确保一个应用显式地为所有 I/O 操作设置正确地编码，strict 模式是最安全的，但是产生一个异常时，这种模式可能导致程序崩溃。
    另外一些错误模式更为灵活。例如，replace 确保不会产生任何错误，其代价是可能丢失一些无法转换为所需编码的数据。pi（π）的 Unicode 字符仍然无法用 ASCII 编码，但是采用这种错误处理模式时，并不是产生一个异常，而是会在输出中将这个字符替换为 ?。
    要完全跳过有问题的数据，可以使用 ignore。无法编码的数据都会被丢弃。
    还有两种无损的错误处理选项，这两种模式会用一个不同于编码的标准中定义的候选表示替换字符。xmlcharrefreplace 使用一个 XML 字符引用来完成替换（W3C 文档《XML Entity Definitions for Characters》（字符的 XML 实体定义）中指定了字符引用列表）。另一个无损的错误处理机制是 backslashreplace，它生成的输入格式类似于打印 unicode 对象的 repr() 时返回的值。Unicode 字符会替换为 \u，后面会跟有码点的十六进制值。
    2. 解码错误
    数据解码时也有可能遇到错误，特别是在使用了错误的编码时。

import codecs
import sys

from codecs_to_hex import to_hex

error_handling = sys.argv[1]

text = u'pi: \u03c0'
print 'Original      :', repr(text)

# Save the data with one encoding
with codecs.open('decode_error.txt', 'w', encoding='utf-16') as f:
    f.write(text)

# Dump the bytes from the file
with open('decode_error.txt', 'r') as f:
    print 'File contents:', to_hex(f.read(), 1)

# Try to read the data with the wrong encoding
with codecs.open('decode_error.txt', 'r',
                 encoding='utf-8',
                 errors=error_handling) as f:
    try:
        data = f.read()
    except UnicodeDecodeError, err:
        print 'ERROR:', err
    else:
        print 'Read          :', repr(data)

    与编码一样，如果不能正确地解码字节流，strict 错误处理模式会产生一个异常。在这种情况下，产生 UnicodeDecodeError 的原因是尝试将 UTF-16 BOM 的部分转换为一个使用 UTF-8 解码器的字符。
    切换到 ignore 会导致解码器跳过不合法的字节。不过，结果仍然不是原来期望的，因为其中包括嵌入的 null 字节。
    采用 replace 模式时，非法的字节会替换为 \uFFFD，这是官方的 Unicode 替换字符，看起来像是一个有黑色背景的菱形，其中包含一个白色的问号。
标准输入和输出流
    之所以会产生 UnicodeEncodeError 异常，最常见的原因是代码中试图将 unicode 数据打印到控制台或一个 UNIX 管道，而 sys.stdout 未配置一个编码。

import codecs
import sys

text = u'pi: π'
# Print to stdout may cause an encoding error
print 'Defalut encoding:', sys.stdout.encoding
print 'TTY:', sys.stdout.isatty()
print text

    标准 I/O 通道默认编码存在的问题可能很难调试。这是因为，向控制台打印输出时，程序通常仍能如期工作，但是作为管道的一部分而且输出包括超出 ASCII 范围的 Unicode 字符时，则会导致一个编码错误。这种行为差异是 Python 的初始化代码造成的，它只在通道连接到一个终端时（isatty() 返回 True）才会为各个标准 I/O 通道设置默认编码。如果没有终端，Python 则假设程序会显式地配置编码，因此不再设置 I/O 通道的编码。
    要显式地设置标准输出通道的编码，可以使用 getwriter() 得到一个特定编码的流编码器类。实例化这个类，传入 sys.stdout 作为其唯一参数。

import codecs
import sys

text = u'pi: π'

# Wrap sys.stdout with a writer that knows how to handle encoding
# Unicode data.
wrapped_stdout = codecs.getwriter('UTF-8')(sys.stdout)
wrapped_stdout.write(u'Via write: ' + text + '\n')

# Replace sys.stdout with a writer
sys.stdout = wrapped_stdout

print u'Via print:', text

    写至包装的 sys.stdout 时，将编码字节发送到 stdout 之前会通过一个编码器传递 Unicode 文本。替换 sys.stdout 意味着应用中完成标准输出打印所使用的代码都能利用这个编码书写器。
    下一个要解决的问题时如何知道应当使用哪一个编码。根据位置、语言以及用户或系统配置，可能会有不同的使用编码，所以在程序中硬编码一个固定值不是一个好的想法。对于用户来说，如果需要设置输入和输出编码从而向各个程序传递显式参数，这也很烦人。幸运的是，对此有一种全局方法，可以使用 locale 得到一个合理的默认编码。

import codecs
import locale
import sys

text = u'pi: π'

# Configure locale from the user's enviroment settings.
locale.setlocale(locale.LC_ALL, '')

# Wrap stdout with an encoding-aware writer.
lang, encoding = locale.getdefaultlocale()
print 'Locale encoding    :', encoding
sys.stdout = codecs.getwriter(encoding)(sys.stdout)

print 'With wrapped stdout', text

    函数 locale.getdefaultlocale() 会根据系统和用户的配置设置返回语言和首选编码，并采用一种 getwriter() 可用的格式。
    处理 sys.stdin 时也需要设置编码。使用 getreader() 可以得到一个能够解码输入字节的阅读器。

import codecs
import locale
import sys

# Configure locale from the user's enviroment settings.
locale.setlocale(locale.LC_ALL, '')

# Wrap stdout with an encoding-aware writer.
lang, encoding = locale.getdefaultlocale()
sys.stdin = codecs.getwriter(encoding)(sys.stdin)

print 'From stdin:'
print repr(sys.stdin.read())

    从包装的句柄读取数据时会返回 unicode 对象而不是 str 实例。
编码转换
    尽管大多数应用都在内部处理 unicode 数据，将数据解码或编码作为 I/O 操作的一部分，但有些情况下，可能需要改变文件的编码而不固定这种中间数据格式。EncodeFile() 取一个使用某种编码的打开文件句柄，用一个类来包装这个文件句柄，有 I/O 操作时则会把数据转换为一种编码。

from codecs_to_hex import to_hex

import codecs
from cStringIO import StringIO

# Raw version of the original data.
data = u'pi: \u03c0'

# Manually encode it as UTF-8.
utf8 = data.encode('utf-8')
print 'Start as UTF-8   :', to_hex(utf8, 1)

# Set up an output buffer, then wrap it as an EncodedFile.
output = StringIO()
encoded_file = codecs.EncodedFile(output, data_encoding='utf-8',
                                  file_encoding='utf-16')
encoded_file.write(utf8)

# Fetch the buffer contents as a UTF-16 encoded byte string
utf16 = output.getvalue()
print 'Encoded to UTF-16:', to_hex(utf16, 2)

# Set up another buffer with the UTF-16 data for reading,
# and wrap it with another EncodedFile.
buffer = StringIO(utf16)
encoded_file = codecs.EncodedFile(buffer, data_encoding='utf-8',
                                  file_encoding='utf-16')

# Read the UTF-8 encoded version of the data.
recoded = encoded_file.read()
print 'Back to UTF-8    :', to_hex(recoded, 1)

    这个例子显示了如何读写 EncodeFile() 返回的不同句柄。不论这个句柄用于读还是写，file_encofing 总是指示打开文件句柄所用的编码（作为第一个参数传入），data_encoding 值则指示通过 read() 和 write() 调用传递数据时所用的编码。
非 Unicode 编码
    尽管之前大多数例子都使用 Unicode 编码，实际上 codecs 还可以用于很多其他数据转换。例如，Python 包含了 codecs 来处理 base-64、bzip2、ROT-13、ZIP 和其他数据格式。

import codecs
from cStringIO import StringIO

buffer = StringIO()
stream = codecs.getwriter('rot_13')(buffer)

text = 'abcdefghijklmnopqrstuvwxyz'

stream.write(text)
stream.flush()

print 'Original:', text
print 'ROT-13  :', buffer.getvalue()

    如果转换可以表述为有单个输入参数的函数，并返回一个字节或 Unicode 串，这样的转换都可以注册为一个 codec。
    使用 codecs 包装一个数据流，可以提供比直接使用 zlib 更简单的接口。

import codecs
from cStringIO import StringIO

from codecs_to_hex import to_hex

buffer = StringIO()
stream = codecs.getwriter('zlib')(buffer)
text = 'abcdefghijklmnopqrstuvwxyz\n' * 50

stream.write(text)
stream.flush()

print 'Original length :', len(text)
compressed_data = buffer.getvalue()
print 'ZIP compressed  :', len(compressed_data)

buffer = StringIO(compressed_data)
stream = codecs.getreader('zlib')(buffer)

first_line = stream.readline()
print 'Read first line :', repr(first_line)

uncompressed_data = first_line + stream.read(0)
print 'Uncompressed    :', len(uncompressed_data)
print 'Same            :', text == uncompressed_data

    并不是所有压缩或编码系统都支持使用 readline() 或 read() 通过流接口读取数据的一部分，因为这需要找到压缩段的末尾来实现解压缩。如果一个程序无法再内存中保存整个解压缩的数据集，可以使用压缩库的增量访问特性，而不是 codecs。
增量编码
    目前提供的一些编码（特别是 bz2 和 zlib）在处理数据流时可能会显著改变数据流的长度。对于大的数据集，这些编码采用增量方式可以更好地处理，即一次只处理一个小数据块。IncrementalEncoder 和 IncrementalDecoder API 就设计用来完成这个任务。

import codecs
import sys
from codecs_to_hex import to_hex

text = 'abcdefghijklmnopqrstuvwxyz\n'
repetitions = 50

print 'Text length :', len(text)
print 'Repetitions :', repetitions
print 'Expected len:', len(text) * repetitions

# Encode the text several times to build up a large amount of data
encoder = codecs.getincrementalencoder('bz2')()
encoded = []

print
print 'Encoding:',
for i in range(repetitions):
    en_c = encoder.encode(text, final = (i==repetitions-1))
    if en_c:
        print '\nEncoded : {} bytes'.format(len(en_c))
        encoded.append(en_c)
    else:
        sys.stdout.write('.')

bytes = ''.join(encoded)
print
print 'Total encoded length:', len(bytes)
print

# Decode the byte string one byte at a time
decoder = codecs.getincrementaldecoder('bz2')()
decoded = []

print 'Decoding:',
for i, b in enumerate(bytes):
    final = (i+1) == len(text)
    c = decoder.decode(b, final)
    if c:
        print '\nDecoded : {} characters'.format(len(c))
        print 'Decoding:',
        decoded.append(c)
    else:
        sys.stdout.write('.')
print
restored = u''.join(decoded)

print
print 'Total uncompressed length:', len(restored)

    每次将数据传递到编码器或解码器时，其内部状态会更新。状态一致时（按照 codec 的定义），会返回数据并重置状态。在此之前，encode() 或 decode() 调用并不返回任何数据。传入最后一部分数据时，参数 final 应当设置为 True，这样 codec 就能知道需要刷新输出所有余下的缓冲数据。
Unicode 数据和网络通信
    类似于标准输入和输出文件描述符，网络套接字也是字节流，因此将 Unicode 数据写至一个套接字之前必须先编码为字节。以下服务器会把接收到的数据回送到发送者。

#coding=utf-8
import sys
import SocketServer

class Echo(SocketServer.BaseRequestHandler):
    def handle(self):
        # Get some bytes and echo them back to the client.
        data = self.request.recv(1024)
        self.request.send(data)
        return

if __name__ == '__main__':
    import codecs
    import socket
    import threading

    address = ('localhost', 0) # let the kernel assign a port
    server = SocketServer.TCPServer(address, Echo)
    ip, port = server.server_address # what port was assigned?

    t = threading.Thread(target=server.serve_forever)
    t.setDaemon(True) # don't hang on exit
    t.start()

    # Connect to the server
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.connect((ip, port))

    # Send the data
    # WRONG: Not encoded first!
    text = u'pi: π'
    len_sent = s.send(text)

    # Receive a response
    response = s.recv(len_sent)
    print repr(response)

    # Clean up
    s.close()
    server.socket.close()

    可以在每个 send() 调用之前对数据显式编码，不过如果少一个 send() 调用就会导致一个编码错误。
    可以使用 makefile() 得到套接字的一个类文件句柄，然后用一个基于流的阅读器或书写器包装这个句柄，这意味着 Unicode 串传入和传出套接字时会被编码。

#coding=utf-8
import sys
import SocketServer

class Echo(SocketServer.BaseRequestHandler):
    def handle(self):
        # Get some bytes and echo them back to the client.
        data = self.request.recv(1024)
        self.request.send(data)
        return

class PassThrough(object):

    def __init__(self, other):
        self.other = other

    def write(self, data):
        print 'Writing :', repr(data)
        return self.other.write(data)

    def read(self, size=-1):
        print 'Reading :',
        data = self.other.read(size)
        print repr(data)
        return data

    def flush(self):
        return self.other.flush()

    def close(self):
        return self.other.close()

if __name__ == '__main__':
    import codecs
    import socket
    import threading

    address = ('localhost', 0) # let the kernel assign a port
    server = SocketServer.TCPServer(address, Echo)
    ip, port = server.server_address # what port was assigned?

    t = threading.Thread(target=server.serve_forever)
    t.setDaemon(True) # don't hang on exit
    t.start()

    # Connect to the server
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.connect((ip, port))

    # Wrap the socket with a reader and writer.
    read_file = s.makefile('r')
    incoming = codecs.getreader('utf-8')(PassThrough(read_file))
    write_file = s.makefile('w')
    outgoing = codecs.getwriter('utf-8')(PassThrough(write_file))

    # Send the data
    text = u'pi: π'
    print 'Sending :', repr(text)
    outgoing.write(text)
    outgoing.flush()

    # Receive a response
    response = incoming.read()
    print 'Received:', repr(response)

    # Clean up
    s.close()
    server.socket.close()

    这个例子使用 PassThrough 来展示数据在发送之前会进行编码，另外客户端接收响应之后会进行解码。
定义定制编码
    由于 Python 已经提供了大量编制编码，所以一般应用不太可能需要定义定制的编码器或解码器。不过，如果确实有必要，codecs 中包含有很多基类，使你能更容易地定义定制编码。
    第一步是了解编码描述的转换性质。下面的例子将使用一个“invertcaps”编码，它把大写字母转换为小写，把小写字母转换为大写。下面是一个编码函数的简单定义，它会对一个输入字符串完成这个转换：

import string

def invertcaps(text):
    """Return new string with the case of all letters switched."""
    return ''.join( c.upper() if c in string.ascii_lowercase
                    else c.lower() if c in string.ascii_uppercase
                    else c
                    for c in text
                    )

if __name__ == '__main__':
    print invertcaps('ABC.def')
    print invertcaps('abc.DEF')

    在这里，编码器和解码器都是同一个函数（与 ROT-13 类似）。
    尽管很容易理解，但这个实现效率不高，特别是对于非常大的文本串。幸运的是，codecs 包含一些辅助函数可以根据字符映射（character map）创建编码，如 invertcaps 编码。字符映射编码由两个字典构成。编码映射（encoding map）将输入串的字符值装换为输出中的字节值，解码映射（decoding map）则相反。首先创建解码映射，然后使用 make_encoding_map() 把它转换为一个编码映射。C 函数 charmap_encode() 和 charmap_decode() 可以使用这些映射高效地转换输入数据。

import codecs
import string

# Map every character to itself
decoding_map = codecs.make_identity_dict(range(256))

# Make a list of pairs of ordinal values for the lower and uppercase letters
pairs = zip([ ord(c) for c in string.ascii_lowercase],
            [ ord(c) for c in string.ascii_uppercase])

# Modify the mapping to convert upper to lower and lower to upper.
decoding_map.update( dict( (upper, lower)
                           for (lower, upper)
                           in pairs
                           )
                     )
decoding_map.update( dict( (lower, upper)
                           for (lower, upper)
                           in pairs
                           )
                     )

# Create a separate encoding map.
encoding_map = codecs.make_encoding_map(decoding_map)

if __name__ == '__main__':
    print codecs.charmap_encode('abc.DEF', 'strict', encoding_map)
    print codecs.charmap_decode('abc.DEF', 'strict', decoding_map)
    print encoding_map == decoding_map

    尽管 invertcaps 的编码和解码映射是一样的，但并不总是如此。make_encoding_map() 会检测哪些情况下多个输入字符编码为相同的输出字节，并把编码值替换为 None，将编码标记为未定义。
    字符映射编码器和解码器支持前面介绍的所有标准错误处理方法，所以不需要为支持这部分 API 做任何额外的工作。

import codecs
from codecs_invertcaps_charmap import encoding_map

text = u'pi: π'

for error in [ 'ignore', 'replace', 'strict' ]:
    try:
        encoded = codecs.charmap_encode(text, error, encoding_map)
    except UnicodeEncodeError, err:
        encoded = str(err)
    print '{:7}: {}'.format(error, encoded)

    由于 π 的 Unicode 码点不在编码映射中，所以采用 strict 错误处理模式时会产生一个异常。
    定义了编码和解码映射之后，还需要建立一些额外的类，另外要注册编码。register() 向注册表增加一个搜索函数，这样当用户希望使用这种编码时，codecs 能够找到。这个搜索函数必须有一个字符串参数，其中包含编码名，如果它知道这个编码则返回一个 CodecInfo 对象，否则返回 None。

import codecs
import encodings

def search1(encoding):
    print 'search1: Searching for:', encoding
    return None
def search2(encoding):
    print 'search2: Searching for:', encoding
    return None

codecs.register(search1)
codecs.register(search2)

utf8 = codecs.lookup('utf-8')
print 'UTF-8:', utf8

try:
    unknown = codecs.lookup('no-such-encoding')
except LookupError, err:
    print 'ERROR:', err

    可以注册多个搜索函数，每个搜索函数将依次调用，直到一个搜索函数返回一个 CodecInfo，或者所有搜索函数都已经调用。codecs 注册的内部搜索函数知道如何加载标准编码，如 encodings 的 UTF-8，所以这些编码名不会传递到定制搜索函数。
    搜索函数返回的 CodecInfo 实例告诉 codecs 如何使用所支持的各种不同机制来完成编码和解码，包括：无状态编码、增量式编码和流编码。codecs 包括一些基类来帮助建立字符映射编码。下面这个例子集成了所有内容，它会注册一个搜索函数，并返回为 invertcaps 编码配置的一个 CodecInfo 实例。

import codecs
from codecs_invertcaps_charmap import encoding_map, decoding_map

# Stateless encoder/decoder

class InvertCapsCodec(codecs.Codec):
    def encode(self, input, errors='strict'):
        return codecs.charmap_encode(input, errors, encoding_map)
    def decode(self, input, errors='strict'):
        return codecs.charmap_decode(input, errors, decoding_map)

# Incremental forms

class InvertCapsIncrementalEncoder(codecs.IncrementalEncoder):
    def encode(self, input, final=False):
        data, nbytes = codecs.charmap_encode(input,
                                             self.errors,
                                             encoding_map)
        return data

class InvertCapsIncrementalDecoder(codecs.IncrementalDecoder):
    def decode(self, input, final=False):
        data, nbytes = codecs.charmap_decode(input,
                                             self.errors,
                                             decoding_map)
        return data

# Stream reader and writer

class InvertCapsStreamReader(InvertCapsCodec, codecs.StreamReader):
    pass

class InvertCapsStreamWriter(InvertCapsCodec, codecs.StreamWriter):
    pass

# Register the codec search function

def find_invertcaps(encoding):
    """Return the codec for 'invertcaps'."""
    if encoding == 'invertcaps':
        return codecs.CodecInfo(
            name='invertcaps',
            encode=InvertCapsCodec().encode,
            decode=InvertCapsCodec().decode,
            incrementalencoder=InvertCapsIncrementalEncoder,
            incrementaldecoder=InvertCapsIncrementalDecoder,
            streamreader=InvertCapsStreamReader,
            streamwriter=InvertCapsStreamWriter,
            )
    return None
codecs.register(find_invertcaps)

if __name__ == '__main__':

    # Stateless encoder/decoder
    encoder = codecs.getencoder('invertcaps')
    text = 'abc.DEF'
    encoded_text, consumed = encoder(text)
    print 'Encoded "{}" to "{}", consuming {} characters'.format(
        text, encoded_text, consumed)

    # Stream writer
    import sys
    writer = codecs.getwriter('invertcaps')(sys.stdout)
    print 'StreamWriter for stdout: ',
    writer.write('abc.DEF')
    print

    # Incremental decoder
    decoder_factory = codecs.getincrementaldecoder('invertcaps')
    decoder = decoder_factory()
    decoded_text_parts = []
    for c in encoded_text:
        decoded_text_parts.append(decoder.decode(c, final=False))
    decoded_text_parts.append(decoder.decode('', final=True))
    decoded_text = ''.join(decoded_text_parts)
    print 'IncrementalDecoder converted "{}" to "{}"'.format(
        encoded_text, decoded_text)

    无状态编码器/解码器的基类是 Codec，要用新实现覆盖 encode() 和 decode()（在这里分别调用了 charmap_encode() 和 charmap_decode()）。这些方法必须分别返回一个 tuple，其中包含转换的数据和利用过的输入字节或字符数。charmap_encode() 和 charmap_decode() 已经返回了这个信息，所以很方便。
    IncrementalEncoder 和 IncrementalDecoder 可以作为增量式编码接口的基类。增量类的 encode() 和 decode() 方法定义为只返回具体的转换数据。有关缓冲的所有信息都作为内部状态维护。invertcaps 编码不需要缓冲数据（它使用一种一对一映射）。如果编码根据所处理的数据会生成数量不同的输出，如压缩算法，对于这些编码，BufferedIncrementalEncoder 和 BufferedIncrementalDecoder 作为基类更为合适，因为它们可以管理输入中未处理的部分。
    StreamReader 和 StreamWriter 也需要 encode() 和 decode() 方法，而且因为它们往往返回与 Codec 相应方法同样的值，所以实现时可以使用多重继承。