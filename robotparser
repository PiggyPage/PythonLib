[Python标准库]robotparser——网络蜘蛛访问控制
    作用：解析用于控制网络蜘蛛的 robots.txt 文件。
    Python 版本：2.1.3 及以后版本
    robotparser 为 robots.txt 文件格式实现一个解析器，包括一个函数来检查给定的 user-agent 是否可以访问一个资源。这个模块可以用于需要抑制或限制的合法蜘蛛或其他爬虫应用。
robots.txt
    robots.txt 文件格式是一个基于文本的简单访问控制系统，用于自动访问 Web 资源的计算机程序（如“蜘蛛”、“爬虫”等等）。这个文件由记录构成，各记录会指定程序的 user-agent 标识符，后面是代理不能访问的一个 URL（或 URL 前缀）列表。
    以下是 http://www.doughellmann.com/ 的 robots.txt 文件。
    User-agent: *
    Disallow: /admin/
    Disallow: /downloads/
    Disallow: /media/
    Disallow: /static/
    Disallow: /codehosting/
    这会阻止访问网站中某些计算代价昂贵的部分，如果搜索引擎视图索引这些部分，可能会使服务器负载过重。要得到 robots.txt 更为完整的示例集，可以参考 Web Robots 页面（http://www.robotstxt.org/orig.html）。
测试访问权限
    基于之前提供的测试，一个简单的爬虫应用可以使用 RobotFileParser.can_fetch() 测试是否允许下载一个页面。

import robotparser
import urlparse

AGENT_NAME = 'PyMOTW'
URL_BASE = 'http://www.doughellmann.com/'
parser = robotparser.RobotFileParser()
parser.set_url(urlparse.urljoin(URL_BASE, 'robot.txt'))
parser.read()

PATHS = [
    '/',
    '/PyMOTW',
    '/admin/',
    '/downloads/PyMOTW-1.92.tar.gz',
    ]

for path in PATHS:
	print '%6s : %s' % (parser.can_fetch(AGENT_NAME, path), path)
	url = urlparse.urljoin(URL_BASE, path)
	print '%6s : %s' % (parser.can_fetch(AGENT_NAME, url), url)
	print

    can_fetch() 的一个 URL 参数可以是一个相对于网站根目录的相对路径，也可以是一个完全 URL。
长久蜘蛛
    如果一个应用需要花很长时间来处理它下载的资源，或者受到抑制，需要在多次下载之间暂停，这样的应用应当以其已下载内容的寿命为根据，定期检查新的 robots.txt 文件。这个寿命并不是自动管理的，不过模块提供了一些简单方法，利用这些方法可以更容易地跟踪文件的寿命。

import robotparser
import urlparse
import time

AGENT_NAME = 'PyMOTW'
parser = robotparser.RobotFileParser()
# Using the local copy
parser.set_url('robot.txt')
parser.read()
parser.modified()

PATHS = [
    '/',
    '/PyMOTW',
    '/admin/',
    '/downloads/PyMOTW-1.92.tar.gz',
    ]

for path in PATHS:
	age = int(time.time() - parser.mtime())
	print 'age:', age,
	if age > 1:
		print 'rereading robot.txt'
		parser.read()
		parser.modified()
	else:
		print
	print '%6s : %s' % (parser.can_fetch(AGENT_NAME, path), path)
	# Simulate a delay in processing
	time.sleep(1)
	print

    如果已下载的文件寿命超过了 1 秒，这个极端例子就会下载一个新的 robots.txt 文件。
    作为一个更好的长久应用，在下载整个文件之前可能会请求文件的修改时间。另一方面，robots.txt 文件通常很小，所以再次获取整个文档的开销并不昂贵。
