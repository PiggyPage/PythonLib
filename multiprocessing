[Python标准库]multiprocessing——像线程一样管理进程
    作用：提供一个 API 来管理进程。
    Python 版本：2.6 及以后版本
    multiprocessing 模块包含一个 API，它基于 threading API 可以在多个进程间划分工作。有些情况下，multiprocessing 可以作为临时替换，取代 threading 来利用多个 CPU 内核，避免 Python 全局解释器锁所带来的计算瓶颈。
    由于这种类似性，这里的前几个例子都由 threading 例子修改得来。multiprocessing 中有而 threading 未提供的特性将在后面介绍。
multiprocessing 基础
    要创建第二个进程，最简单的方法是用一个目标函数实例化一个 Process 对象，并调用 start() 让它开始工作。

import multiprocessing

def worker():
    """Worker function"""
    print 'Worker'
    return

if __name__ == '__main__':
    jobs = []
    for i in range(5):
        p = multiprocessing.Process(target=worker)
        jobs.append(p)
        p.start()

    输出中单词“Worker”将打印 5 次，不过不能清楚地看出孰先孰后，这取决于具体的执行顺序，因为每个进程都在竞争访问输出流。
    更有用的做法是，创建一个进程时可以提供参数来告诉它要做什么。与 threading 不同，要向一个 multiprocessing Process 传递参数，这个参数必须能够使用 pickle 串行化。下面这个例子向各个工作进程传递一个要打印的数。

import multiprocessing

def worker(num):
    """Worker function"""
    print 'Worker', num
    return

if __name__ == '__main__':
    jobs = []
    for i in range(5):
        p = multiprocessing.Process(target=worker, args=(i,))
        jobs.append(p)
        p.start()

    现在整数参数会包含在各个工作进程打印的消息中。
可导入的目标函数
    threading 与 multiprocessing 例子之间有一个区别，multiprocessing 例子中对 __main__ 使用了额外的保护。由于新进程启动的方式，要求子进程能够导入包含目标函数的脚本。可以将应用的主要部分包装在一个 __main__ 检查中，确保模块导入时不会在各个子进程中递归地运行。另一种方法是从一个单独的脚本导入目标函数。例如，multiprocessing_import_main.py 使用了第二个模块中定义的一个工作函数。

import multiprocessing
import multiprocessing_import_worker

if __name__ == '__main__':
    jobs = []
    for i in range(5):
        p = multiprocessing.Process(
            target=multiprocessing_import_worker.worker,
            )
        jobs.append(p)
        p.start()

    这个工作函数在 multiprocessing_import_worker.py 中定义。
    调用主程序会生成与第一个例子类似的输出。
确定当前进程
    传递参数来标识或命名进程很麻烦，也没有必要。每个 Process 实例都有一个名称，其默认值可以在创建进程时改变。给进程命名对于跟踪进程很有用，特别是当应用中有多种类型的进程在同时运行时。

import multiprocessing
import time

def worker():
    name = multiprocessing.current_process().name
    print name, 'Starting'
    time.sleep(2)
    print name, 'Exiting'

def my_service():
    name = multiprocessing.current_process().name
    print name, 'Starting'
    time.sleep(3)
    print name, 'Exiting'

if __name__ == '__main__':
    service = multiprocessing.Process(name='my_service',
                                      target=my_service)
    worker_1 = multiprocessing.Process(name='worker_1',
                                       target=worker)
    worker_2 = multiprocessing.Process(target=worker) # default name

    worker_1.start()
    worker_2.start()
    service.start()

    调试输出中，每行都包含当前进程的名称。进程名称列为 Process-3 的行对应未命名的进程 worker_2。
守护进程
    默认情况下，在所有子进程退出之前主程序不会退出。有些情况下，可能需要启动一个后台进程，它可以一直运行而不阻塞主程序退出，如果一个服务无法用一种容易地方法来中断进程，或者希望进程工作到一半时中止而不损失或破坏数据（如为一个服务监控工具生成“心跳”的任务），对于这些服务，使用守护进程就很有用。
    要标志一个进程为守护进程，可以将其 daemon 属性设置为 True。默认情况下进程不作为守护进程。

import multiprocessing
import time
import sys

def daemon():
    p = multiprocessing.current_process()
    print 'Starting:', p.name, p.pid
    sys.stdout.flush()
    time.sleep(2)
    print 'Exiting :', p.name, p.pid
    sys.stdout.flush()

def non_daemon():
    p = multiprocessing.current_process()
    print 'Starting:', p.name, p.pid
    sys.stdout.flush()
    time.sleep(2)
    print 'Exiting :', p.name, p.pid
    sys.stdout.flush()

if __name__ == '__main__':
    d = multiprocessing.Process(name='daemon', target=daemon)
    d.daemon = True

    n = multiprocessing.Process(name='non-daemon', target=non_daemon)
    n.daemon = False

    d.start()
    time.sleep(1)
    n.start()

    输出中没有守护进程的“Exiting”消息，因为在守护进程从其 2 秒的睡眠时间唤醒之前，所有非守护进程（包括主程序）已经退出。
    守护进程会在主进程退出之前自动终止，以避免留下“孤”进程继续运行。要验证这一点，可以查找程序运行时打印的进程 id 值，然后用一个类似 ps 的命令检查该进程。
等待进程
    要等待一个进程完成工作并退出，可以使用 join() 方法。

import multiprocessing
import time
import sys

def daemon():
    name = multiprocessing.current_process().name
    print 'Starting:', name
    time.sleep(2)
    print 'Exiting :', name

def non_daemon():
    name = multiprocessing.current_process().name
    print 'Starting:', name
    print 'Exiting :', name

if __name__ == '__main__':
    d = multiprocessing.Process(name='daemon', target=daemon)
    d.daemon = True

    n = multiprocessing.Process(name='non-daemon', target=non_daemon)
    n.daemon = False

    d.start()
    time.sleep(1)
    n.start()

    d.join()
    n.join()

    由于主进程使用 join() 等待守护进程退出，所以这一次会打印“Exiting”消息。
    默认情况下，join() 会无限阻塞。可以传入一个超时参数（这是一个浮点数，表示在进程变为不活动之前所等待的秒数）。即使进程在这个超时期限内没有完成，join() 也会返回。

import multiprocessing
import time
import sys

def daemon():
    name = multiprocessing.current_process().name
    print 'Starting:', name
    time.sleep(2)
    print 'Exiting :', name

def non_daemon():
    name = multiprocessing.current_process().name
    print 'Starting:', name
    print 'Exiting :', name

if __name__ == '__main__':
    d = multiprocessing.Process(name='daemon', target=daemon)
    d.daemon = True

    n = multiprocessing.Process(name='non-daemon', target=non_daemon)
    n.daemon = False

    d.start()
    n.start()
    d.join(1)
    print 'd.is_alive()', d.is_alive()
    n.join()

    由于传入的超时值小于守护进程睡眠的时间，所以 join() 返回之后这个进程仍“存活”。
终止进程
    尽管最好使用“毒丸”（poison pill）方法向进程发出信号告诉它应当退出，但是如果一个进程看起来已经挂起或陷入死锁，则需要能够强制性地将其结束。对一个进程对象调用 terminate() 会结束子进程。

import multiprocessing
import time

def slow_worker():
    print 'Starting worker'
    time.sleep(0.1)
    print 'Finished worker'
    
if __name__ == '__main__':
    p = multiprocessing.Process(target=slow_worker)
    print 'BEFORE:', p, p.is_alive()

    p.start()
    print 'DURING:', p, p.is_alive()

    p.terminate()
    print 'TERMINATED:', p, p.is_alive()

    p.join()
    print 'JOINED:', p, p.is_alive()

进程退出状态
    进程退出时生成的状态码可以通过 exitcode 属性访问。下表列出了可用的退出码范围。
    --------------------------------------------------------
        退出码    |                 含义
    --------------------------------------------------------
        == 0      |    未生成任何错误
    --------------------------------------------------------
        > 0       |    进程有一个错误，并以该错误码退出
    --------------------------------------------------------
        < 0       |    进程由一个 -1 * exitcode 信号结束
    --------------------------------------------------------

import multiprocessing
import sys
import time

def exit_error():
    sys.exit(1)

def exit_ok():
    return

def return_value():
    return 1

def raises():
    raise RuntimeError('There was an error!')

def terminated():
    time.sleep(3)

if __name__ == '__main__':
    jobs = []
    for f in [exit_error, exit_ok, return_value, raises, terminated]:
        print 'Starting process for', f.func_name
        j = multiprocessing.Process(target=f, name=f.func_name)
        jobs.append(j)
        j.start()

    jobs[-1].terminate()

    for j in jobs:
        j.join()
        print '%15s.exitcode = %s' % (j.name, j.exitcode)

    产生异常的进程会自动得到 exitcode 为 1。
日志
    调试并发问题时，能够访问 multiprocessing 提供的对象的内部状态会很有用。可以使用一个方便的模块级函数来启用日志记录，名为 log_to_stderr()。它使用 logging 建立一个日志记录器对象，并增加一个处理程序，使得日志消息将发送到标准错误通道。

import multiprocessing
import sys
import logging

def worker():
    print 'Doing some work'
    sys.stdout.flush()

if __name__ == '__main__':
    multiprocessing.log_to_stderr(logging.DEBUG)
    p = multiprocessing.Process(target=worker)
    p.start()
    p.join()

    默认情况下，日志级别设置为 NOTSET，即不产生任何消息。通过传入一个不同的日志级别，可以初始化日志记录器，指定所需的消息程度。
    要直接处理日志记录器（修改其日志级别或添加处理程序），可以使用 get_logger()。

import multiprocessing
import sys
import logging

def worker():
    print 'Doing some work'
    sys.stdout.flush()

if __name__ == '__main__':
    multiprocessing.log_to_stderr()
    logger = multiprocessing.get_logger()
    logger.setLevel(logging.INFO)
    p = multiprocessing.Process(target=worker)
    p.start()
    p.join()

    使用名称 multiprocessing，还可以通过 logging 配置文件 API 来配置日志记录器。
派生进程
    要在一个单独的进程中开始工作，尽管最简单的方法是使用 Process 并传入一个目标函数，不过也可以使用一个定制子类。

import multiprocessing

class Worker(multiprocessing.Process):

    def run(self):
        print 'In %s' % self.name
        return
    
if __name__ == '__main__':
    jobs = []
    for i in range(5):
        p = Worker()
        jobs.append(p)
        p.start()
    for j in jobs:
        j.join()

    派生类应当覆盖 run() 来完成工作。
向进程传递消息
    类似于线程，对于多个进程，一种常用的模式是将一个工作划分到多个工作进程中并行地运行。要想有效地使用多个进程，通常要求它们之间有某种通信，这样才能分解工作，并完成结果的汇总。利用 multiprocessing 完成进程间通信的一种简单方法是使用一个 Queue 来回传递消息。能够用 pickle 串行化的任何对象都可以通过 Queue 传递。

import multiprocessing

class MyFancyClass(object):

    def __init__(self, name):
        self.name = name

    def do_something(self):
        proc_name = multiprocessing.current_process().name
        print 'Doing something fancy in %s for %s!' % \
              (proc_name, self.name)

def worker(q):
    obj = q.get()
    obj.do_something()

if __name__ == '__main__':
    queue = multiprocessing.Queue()
    p = multiprocessing.Process(target=worker, args=(queue,))
    p.start()

    queue.put(MyFancyClass('Fancy Dan'))

    # Wait for the worker to finish
    queue.close()
    queue.join_thread()
    p.join()

    这个小例子只是向一个工作进程传递一个消息，然后主进程等待这个工作进程完成。
    来看一个更复杂的例子，这里展示了如何管理多个工作进程，它们都利用一个 JoinableQueue 的数据，并把结果传递回父进程。这里使用“毒丸”技术来停止工作进程。建立具体任务后，主程序会在作业队列中为每个工作进程添加一个“stop”值。当一个工作进程遇到这个特定值时，就会退出其处理循环。子进程使用任务队列的 join() 方法等待所有任务都完成后才开始处理结果。

import multiprocessing
import time

class Consumer(multiprocessing.Process):

    def __init__(self, task_queue, result_queue):
        multiprocessing.Process.__init__(self)
        self.task_queue = task_queue
        self.result_queue = result_queue

    def run(self):
        proc_name = self.name
        while True:
            next_task = self.task_queue.get()
            if next_task is None:
                # Poison pill means shutdown
                print '%s: Exiting' % proc_name
                self.task_queue.task_done()
                break
            print '%s: %s' % (proc_name, next_task)
            answer = next_task()
            self.task_queue.task_done()
            self.result_queue.put(answer)
        return

class Task(object):
    def __init__(self, a, b):
        self.a = a
        self.b = b
    def __call__(self):
        time.sleep(0.1) # pretend to take some time to do the work
        return '%s * %s = %s' % (self.a, self.b, self.a * self.b)
    def __str__(self):
        return '%s * %s' % (self.a, self.b)

if __name__ == '__main__':
    # Establish communication queues
    tasks = multiprocessing.JoinableQueue()
    results = multiprocessing.Queue()

    # Start consumers
    num_consumers = multiprocessing.cpu_count() * 2
    print 'Creating %d consumers' % num_consumers
    consumers = [ Consumer(tasks, results)
                  for i in xrange(num_consumers) ]
    for w in consumers:
        w.start()

    # Enqueue jobs
    num_jobs = 10
    for i in xrange(num_jobs):
        tasks.put(Task(i, i))

    # Add a poison pill for each consumer
    for i in xrange(num_consumers):
        tasks.put(None)
    # Wait for all the tasks to finish
    tasks.join()

    # Start printing results
    while num_jobs:
        result = results.get()
        print 'Result:', result
        num_jobs -= 1

    尽管作业按顺序进入队列，但它们的执行却是并行的，所以不能保证它们完成的顺序。
进程间信号传输
    Event 类提供了一种简单的方法，可以在进程之间传递状态信息。事件可以切换设置和未设置状态。通过使用一个可选的超时值，事件对象的用户可以等待其状态从未设置变为设置。

import multiprocessing
import time

def wait_for_event(e):
    """Wait for the event to be set before doing anything"""
    print 'wait_for_event: starting'
    e.wait()
    print 'wait_for_event: e.is_set()->', e.is_set()

def wait_for_event_timeout(e, t):
    """Wait t seconds and then timeout"""
    print 'wait_for_event_timeout: starting'
    e.wait(t)
    print 'wait_for_event_timeout: e.is_set()->', e.is_set()

if __name__ == '__main__':
    e = multiprocessing.Event()
    w1 = multiprocessing.Process(name='block',
                                 target=wait_for_event,
                                 args=(e,))
    w1.start()

    w2 = multiprocessing.Process(name='nonblock',
                                 target=wait_for_event_timeout,
                                 args=(e, 2))
    w2.start()

    print 'main: waiting before calling Event.set()'
    time.sleep(3)
    e.set()
    print 'main: event is set'

    wait() 到时间时就会返回，而没有任何错误。调用者负责使用 is_set() 检查事件的状态。
控制资源访问
    如果需要在多个进程间共享一个资源，在这种情况下，可以使用一个 Lock 来避免访问冲突。

import multiprocessing
import sys

def worker_with(lock, stream):
    with lock:
        stream.write('Lock acquired via with\n')

def worker_no_with(lock, stream):
    lock.acquire()
    try:
        stream.write('Lock acquired directly\n')
    finally:
        lock.release()

lock = multiprocessing.Lock()
w = multiprocessing.Process(target=worker_with,
                            args=(lock, sys.stdout))
nw = multiprocessing.Process(target=worker_no_with,
                             args=(lock, sys.stdout))
w.start()
nw.start()

w.join()
nw.join()

    在这个例子中，如果这两个进程没有用锁同步其输出流访问，打印到控制台的消息可能会纠结在一起。
同步操作
    Condition 对象可以用来同步一个工作流的各个部分，使其中一些部分并行运行，而另外一些顺序运行，即使它们在单独的进程中。

import multiprocessing
import time

def stage_1(cond):
    """perform first stage of work,
    then notify stage_2 to continue
    """
    name = multiprocessing.current_process().name
    print 'Starting', name
    with cond:
        print '%s done and ready for stage 2' % name
        cond.notify_all()

def stage_2(cond):
    """wait for the condition telling us stage_1 is done"""
    name = multiprocessing.current_process().name
    print 'Starting', name
    with cond:
        cond.wait()
        print '%s running' % name

if __name__ == '__main__':
    condition = multiprocessing.Condition()
    s1 = multiprocessing.Process(name='s1',
                                 target=stage_1,
                                 args=(condition,))
    s2_clients = [
        multiprocessing.Process(name='stage_2[%d]' % i,
                                target=stage_2,
                                args=(condition,))
        for i in range(1, 3)
        ]
    for c in s2_clients:
        c.start()
        time.sleep(1)
    s1.start()

    s1.join()
    for c in s2_clients:
        c.join()

    在这个例子中，两个进程并行地运行一个作业的第二阶段，但前提是第一阶段已经完成。
控制资源的并发访问
    有时可能需要运行多个工作进程同时访问一个资源，但要限制总数。例如，连接池支持同时连接，但数目可能是固定的，或者一个网络应用可能支持固定数目的并发下载。这些连接就可以使用 Semaphore 来管理。

import multiprocessing
import time
import random

class ActivePool(object):
    def __init__(self):
        super(ActivePool, self).__init__()
        self.mgr = multiprocessing.Manager()
        self.active = self.mgr.list()
        self.lock = multiprocessing.Lock()
    def makeActive(self, name):
        with self.lock:
            self.active.append(name)
    def makeInactive(self, name):
        with self.lock:
            self.active.remove(name)
    def __str__(self):
        with self.lock:
            return str(self.active)

def worker(s, pool):
    name = multiprocessing.current_process().name
    with s:
        pool.makeActive(name)
        print 'Now running: %s' % str(pool)
        time.sleep(random.random())
        pool.makeInactive(name)

if __name__ == '__main__':
    pool = ActivePool()
    s = multiprocessing.Semaphore(3)
    jobs = [
        multiprocessing.Process(name=str(i),
                                target=worker,
                                args=(s, pool))
        for i in range(10)
        ]

    for j in jobs:
        j.start()

    for j in jobs:
        j.join()
        print 'Now running: %s' % str(pool)

    在这个例子中，ActivePool 类只作为一种便利方法，用来跟踪某个给定时刻哪些进程能够运行。真正的资源池会为新的活动进程分配一个连接或另外某个值，这些进程工作完成时再回收这个值。在这里，资源池只是用来保存活动进程的名称，以显示至少有三个进程在并发运行。
管理共享状态
    在前面的例子中，ActivePool 实例通过一个由 Manager 创建的特殊类型列表对象集中维护活动进程列表。Manager 负责协调其所有用户之间的共享信息状态。

import multiprocessing

def worker(d, key, value):
    d[key] = value

if __name__ == '__main__':
    mgr = multiprocessing.Manager()
    d = mgr.dict()
    jobs = [ multiprocessing.Process(target=worker, args=(d, i, i*2))
             for i in range(10)
             ]
    for j in jobs:
        j.start()
    for j in jobs:
        j.join()
    print 'Results:', d

    通过管理器来创建列表，这个列表将会共享，所有进程都能看到列表更新。除了列表，管理器还支持字典。
共享命名空间
    除了字典和列表之外，Manager 还可以创建一个共享 Namespace。

import multiprocessing

def producer(ns, event):
    ns.value = 'This is the value'
    event.set()

def consumer(ns, event):
    try:
        value = ns.value
    except Exception, err:
        print 'Before event, error:', str(err)
    event.wait()
    print 'After event:', ns.value

if __name__ == '__main__':
    mgr = multiprocessing.Manager()
    namespace = mgr.Namespace()
    event = multiprocessing.Event()
    p = multiprocessing.Process(target=producer,
                                args=(namespace, event))
    c = multiprocessing.Process(target=consumer,
                                args=(namespace, event))
    c.start()
    p.start()
    c.join()
    p.join()

    添加到 Namespace 的所有命名值对所有接收 Namespace 实例的客户都可见。
    要知道重要的一点，对命名空间中可变值内容的更新不会自动传播。

import multiprocessing

def producer(ns, event):
    # DOES NOT UPDATE GLOBAL VALUE!
    ns.my_list.append('This is the value')
    event.set()

def consumer(ns, event):
    print 'Before event:', ns.my_list
    event.wait()
    print 'After event:', ns.my_list

if __name__ == '__main__':
    mgr = multiprocessing.Manager()
    namespace = mgr.Namespace()
    namespace.my_list = []
    
    event = multiprocessing.Event()
    p = multiprocessing.Process(target=producer,
                                args=(namespace, event))
    c = multiprocessing.Process(target=consumer,
                                args=(namespace, event))
    c.start()
    p.start()
    c.join()
    p.join()

    要更新这个列表，需要将它再次关联到命名空间对象。
进程池
    有些情况下，所要完成的工作可以分解并独立地分布到多个工作进程，对于这种简单的情况，可以用 Pool 类来管理固定数目的工作进程。作业的返回值会收集并作为一个列表返回。池（pool）参数包括进程数以及启动任务进程时要运行的函数（对每个子进程调用一次）。

import multiprocessing

def do_calculation(data):
    return data * 2

def start_process():
    print 'Starting', multiprocessing.current_process().name

if __name__ == '__main__':
    inputs = list(range(10))
    print 'Input   :', inputs

    builtin_outputs = map(do_calculation, inputs)
    print 'Built-in:', builtin_outputs

    pool_size = multiprocessing.cpu_count() * 2
    pool = multiprocessing.Pool(processes=pool_size,
                                initializer=start_process,
                                )
    pool_outputs = pool.map(do_calculation, inputs)
    pool.close() # no more tasks
    pool.join()  # wrap up current tasks

    print 'Pool    :', pool_outputs

    map() 方法在功能上等价于内置 map()，只不过单个任务会并行运行。由于进程池并行地处理输入，可以用 close() 和 join() 使任务进程与主进程同步，以确保完成适当的清理。
    默认情况下，Pool 会创建固定数目的工作进程，并向这些工作进程传递作业，直到再没有更多作业为止。设置 maxtasksperchild 参数可以告诉池在完成一些任务之后要重新启动一个工作进程，来避免运行时间很长的工作进程消耗太多的系统资源。

import multiprocessing

def do_calculation(data):
    return data * 2

def start_process():
    print 'Starting', multiprocessing.current_process().name

if __name__ == '__main__':
    inputs = list(range(10))
    print 'Input   :', inputs

    builtin_outputs = map(do_calculation, inputs)
    print 'Built-in:', builtin_outputs

    pool_size = multiprocessing.cpu_count() * 2
    pool = multiprocessing.Pool(processes=pool_size,
                                initializer=start_process,
                                maxtasksperchild=2,
                                )
    pool_outputs = pool.map(do_calculation, inputs)
    pool.close() # no more tasks
    pool.join()  # wrap up current tasks

    print 'Pool    :', pool_outputs

    池完成其所分配的任务时，即使并没有更多工作要做，也会重新启动工作进程。从这个输出可以看到，尽管只有 10 个任务，而且每个工作进程一次可以完成两个任务，但是这里创建了 8 个工作进程。
实现 MapReduce
    Pool 类可以用于创建一个简单的单服务器 MapReduce 实现。尽管它无法充分提供分布处理的好处，但确实展示了将一些问题分解到可分布的工作单元是何等容易。
    在基于 MapReduce 的系统中，输入数据分解为块，由不同的工作进程实例处理。首先使用一个简单的转换将各个输入数据块映射到一个中间状态。然后将中间数据汇集在一起，基于一个键值分区，使所有相关的值都在一起。最后，将分区的数据归约为一个结果集。

import collections
import itertools
import multiprocessing

class SimpleMapReduce(object):
    def __init__(self, map_func, reduce_func, num_workers=None):
        """
        map_func
        Function to map inputs to intermediate data. Takes as
        argument one input value and returns a tuple with the key
        and a value to be reduced.

        reduce_func
        Function to reduce partitioned version of intermediate data
        to final output. Takes as argument a key as produced by
        map_func and a sequence of the values associated with that
        key.
        num_workers
        The number of workers to create in the pool. Defaults to
        the number of CPUs available on the current host.
        """
        self.map_func = map_func
        self.reduce_func = reduce_func
        self.pool = multiprocessing.Pool(num_workers)

    def partition(self, mapped_values):
        """Organize the mapped values by their key.
        Returns an unsorted sequence of tuples with a key
        and a sequence of values.
        """

        partitioned_data = collections.defaultdict(list)
        for key, value in mapped_values:
            partitioned_data[key].append(value)
        return partitioned_data.items()

    def __call__(self, inputs, chunksize=1):
        """Process the inputs through the map and reduce functions given.
        inputs
        An iterable containing the input data to be processed.
        chunksize=1
        The portion of the input data to hand to each worker. This
        can be used to tune performance during the mapping phase.
        """
        map_responses = self.pool.map(self.map_func,
                                      inputs,
                                      chunksize=chunksize)
        partitioned_data = self.partition(
            itertools.chain(*map_responses)
            )
        reduced_values = self.pool.map(self.reduce_func,
                                      partitioned_data)
        return reduced_values

    以下示例脚本使用 SimpleMapReduce 统计这篇文章 reStructureText 源中的“单词”数，这里要忽略其中的一些标记。

import multiprocessing
import string

from multiprocessing_mapreduce import SimpleMapReduce

def file_to_words(filename):
    """Read a file and return a sequence of
    (word, occurrences) values.
    """
    STOP_WORDS = set([
        'a', 'an', 'and', 'are', 'as', 'be', 'by', 'for', 'if',
        'in', 'is', 'it', 'of', 'or', 'py', 'rst', 'that', 'the',
        'to', 'with',
        ])
    TR = string.maketrans(string.punctuation,
                          ' ' * len(string.punctuation))
    print multiprocessing.current_process().name, 'reading', filename
    output = []

    with open(filename, 'rt') as f:
        for line in f:
            if line.lstrip().startswith('..'): # Skip comment lines
                continue
            line = line.translate(TR)  # Strip punctuation
            for word in line.split():
                word = word.lower()
                if word.isalpha() and word not in STOP_WORDS:
                    output.append( (word, 1) )
    return output

def count_words(item):
    """Convert the partitioned data for a word to a
    tuple containing the word and the number of occurrences.
    """
    word, occurrences = item
    return (word, sum(occurrences))

if __name__ == '__main__':
    import operator
    import glob

    input_files = glob.glob('*.rst')

    mapper = SimpleMapReduce(file_to_words, count_words)
    word_counts = mapper(input_files)
    word_counts.sort(key=operator.itemgetter(1))
    word_counts.reverse()

    print '\nTOP 20 WORDS BY FREQUENCY\n'
    top20 = word_counts[:20]
    longest = max(len(word) for word, count in top20)
    for word, count in top20:
        print '%-*s: %5s' % (longest+1, word, count)

    file_to_words() 函数将各个输入文件转换为一个元组序列，各元组包含单词和数字1（表示一次出现）。partition() 使用单词作为键来划分数据，所以得到的结构包括一个键和一个 1 值序列（表示单词的各次出现）。分区数据转换为一组元组，元组中包含一个单词和归约阶段中 count_words() 统计得出的这个单词的出现次数。